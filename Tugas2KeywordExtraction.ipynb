{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ddf0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     --                                       0.1/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "     --                                       0.1/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "     ---                                      0.1/1.5 MB 901.1 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 762.6 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     ----                                     0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     -----                                    0.2/1.5 MB 478.0 kB/s eta 0:00:03\n",
      "     ------                                   0.3/1.5 MB 542.5 kB/s eta 0:00:03\n",
      "     --------                                 0.3/1.5 MB 655.4 kB/s eta 0:00:02\n",
      "     ---------                                0.4/1.5 MB 655.2 kB/s eta 0:00:02\n",
      "     ----------                               0.4/1.5 MB 655.5 kB/s eta 0:00:02\n",
      "     -----------                              0.4/1.5 MB 671.0 kB/s eta 0:00:02\n",
      "     -----------                              0.5/1.5 MB 655.2 kB/s eta 0:00:02\n",
      "     -----------                              0.5/1.5 MB 655.2 kB/s eta 0:00:02\n",
      "     -----------                              0.5/1.5 MB 655.2 kB/s eta 0:00:02\n",
      "     -------------                            0.5/1.5 MB 581.3 kB/s eta 0:00:02\n",
      "     ---------------                          0.6/1.5 MB 655.1 kB/s eta 0:00:02\n",
      "     ----------------                         0.6/1.5 MB 666.4 kB/s eta 0:00:02\n",
      "     -----------------                        0.6/1.5 MB 666.2 kB/s eta 0:00:02\n",
      "     ------------------                       0.7/1.5 MB 686.1 kB/s eta 0:00:02\n",
      "     -------------------                      0.7/1.5 MB 683.9 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 673.4 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 682.7 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 682.7 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 682.7 kB/s eta 0:00:02\n",
      "     --------------------                     0.8/1.5 MB 682.7 kB/s eta 0:00:02\n",
      "     ------------------------                 0.9/1.5 MB 678.1 kB/s eta 0:00:01\n",
      "     -------------------------                1.0/1.5 MB 677.3 kB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 690.7 kB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 675.8 kB/s eta 0:00:01\n",
      "     ---------------------------              1.0/1.5 MB 675.1 kB/s eta 0:00:01\n",
      "     ----------------------------             1.1/1.5 MB 687.9 kB/s eta 0:00:01\n",
      "     -----------------------------            1.1/1.5 MB 686.9 kB/s eta 0:00:01\n",
      "     ------------------------------           1.1/1.5 MB 686.0 kB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.5 MB 679.1 kB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.5 MB 684.6 kB/s eta 0:00:01\n",
      "     ---------------------------------        1.2/1.5 MB 689.5 kB/s eta 0:00:01\n",
      "     ---------------------------------        1.3/1.5 MB 682.7 kB/s eta 0:00:01\n",
      "     ----------------------------------       1.3/1.5 MB 682.0 kB/s eta 0:00:01\n",
      "     -----------------------------------      1.3/1.5 MB 681.6 kB/s eta 0:00:01\n",
      "     ------------------------------------     1.4/1.5 MB 680.9 kB/s eta 0:00:01\n",
      "     ------------------------------------     1.4/1.5 MB 675.3 kB/s eta 0:00:01\n",
      "     -------------------------------------    1.4/1.5 MB 674.8 kB/s eta 0:00:01\n",
      "     --------------------------------------   1.5/1.5 MB 674.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 678.6 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 673.7 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 666.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.3.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "                                              0.0/97.9 kB ? eta -:--:--\n",
      "     ------------                             30.7/97.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -----------------------                61.4/97.9 kB 825.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 97.9/97.9 kB 932.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk) (1.3.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "                                              0.0/268.0 kB ? eta -:--:--\n",
      "     -----------                             81.9/268.0 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------                           92.2/268.0 kB 1.7 MB/s eta 0:00:01\n",
      "     -----------------                      122.9/268.0 kB 1.0 MB/s eta 0:00:01\n",
      "     -----------------------                163.8/268.0 kB 1.1 MB/s eta 0:00:01\n",
      "     -----------------------              174.1/268.0 kB 871.5 kB/s eta 0:00:01\n",
      "     -----------------------              174.1/268.0 kB 871.5 kB/s eta 0:00:01\n",
      "     -----------------------              174.1/268.0 kB 871.5 kB/s eta 0:00:01\n",
      "     -----------------------              174.1/268.0 kB 871.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 268.0/268.0 kB 687.4 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "                                              0.0/77.1 kB ? eta -:--:--\n",
      "     -----                                    10.2/77.1 kB ? eta -:--:--\n",
      "     --------------------                   41.0/77.1 kB 653.6 kB/s eta 0:00:01\n",
      "     -----------------------------------    71.7/77.1 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 77.1/77.1 kB 607.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.6 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba82720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1ea4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Haya\n",
      "[nltk_data]     Dinah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Haya\n",
      "[nltk_data]     Dinah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d01a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Objek wisata yang ada di Indonesia merupakan kekayaan alam yang patut untuk dibanggakan. Setiap daerah di Indonesia memiliki keunikan baik dari segi keindahannya maupun adat istiadat yang ada di daerah tersebut, sehingga menarik minat pengunjung untuk mengunjunginya. Negara Indonesia memiliki banyak objek daya tarik wisata yang sangat potensial dan tidak kalah indahnya dengan di Kota Wonosobo. Sektor pariwisata sebagai kegiatan perekonomian telah menjadi andalan dan prioritas pengembangan bagi sejumlah Negara, terlebih bagi Negara berkembang seperti Indonesia yang memiliki potensi wilayah yang luas. Pengetahuan ini tidak hanya penting bagi pengusaha di bidang pariwisata namun juga diperlukan untuk para generasi muda yang kelak akan mewarisi sebagai pengelola pariwisata Indonesia di masa depan. Banyak tempat yang bisa dijadikan tujuan wisata salah satunya ada di Kota Wonosobo, anda bisa menemukan berbagai pemandangan alam khususnya seperti pegunungan yang sangat indah. Membahas keindahan tempat wisata di Kota Wonosobo, Provinsi Jawa Tengah tidak akan ada habisnya, inilah salah satu kebanggaan yang dimiliki Indonesia, keindahan alam dengan pemandangan yang tidak akan pernah habis untuk dinikmati.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2831993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5910c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "yang\n",
      "di\n",
      "indonesia\n",
      "ada\n",
      "wisata\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd331d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Pada masa sekarang ini, perkembangan teknologi informasi sudah semakin luas dan secara langsung telah mempengaruhi ke dalam setiap aspek kehidupan kita. Bahkan sekarang masyarakat luas sudah memanfaatkan teknologi internet untuk mendorong kemajuan teknologi.Setiap universitas pasti memiliki sistem pengelolaan keuangan untuk pembayaran uang perkuliahan, yang pada umumnya pengelolaan data pembayaran uang kuliah masih dilakukan oleh satu pihak yang terkait yaitu pihak bank dan pengelolaan datanya pun masih dilakukan secara manual oleh pihak lembaga pendidikan tersebut. Terkait dengan hal di atas, salah satu bagian yang tidak terpisahkan dari administrasi suatu lembaga pendidikan adalah sistem pengelolaan data peserta didik yang ada di dalamnya. Pengelolaan data tersebut membutuhkan kecepatan dan keakuratan, sehingga akan memudahkan penentuan data terbaru dengan lebih cepat pula. Melalui pengelolaan berbasis internet, data yang diperlukan akan dapat diterima dengan cepat dan akurat oleh yang membutuhkan, tanpa dibatasi jarak dan waktu. Pengujian unjuk kerja sistem menunjukkan bahwa tiap-tiap modul dan halaman web mampu bekerja sesuai dengan fungsinya. Unjuk kerja sistem adalah mempermudah system pembayaran perkuliahan , mampu menangani otentikasi user, mampu menangani pengelolahan data oleh mahasiswa, mampu menangani pengelolaan data oleh petugas dan sistem mampu menangani pengelolaan data oleh administrator. Penelitian ini bertujuan untuk mengembangkan dan mengetahui unjuk kerja perangkat lunak berbasis web yang digunakan untuk pelaporan pembayaran perkuliahan dan pengolahan data akademik mahasiswa di UIN Syarif Hidayatullah Jakarta.maka dari itu, konsep yang diterapkan dalam tahap perancangan sistem pengolahan data pribadi dan data akademik mahasiswa adalah dengan menggunakan metode pengembangan system SDLC (system Development Life Cycle). Perangkat lunak yang digunakan dalam perancangan dan pembangunan sistem ini, menggunakan PHP dan MySQL\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962d86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2329d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "data\n",
      "pengelolaan\n",
      "dan\n",
      "oleh\n",
      "sistem\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12508bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dcdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Laporan Tugas Akhir dengan judul “SISTEM INFORMASI PENJUALAN BATIK BERBASIS WEB PADA TOKO BATIK Q-TA PEKALONGAN” inimempunyai tujuan untuk merancang sistem informasi penjualan secara online sehingga dapat memudahkan pihak perusahaan dalam memasarkan produk- produknya, membuat laporan penjualan serta memudahkan transaksi penjualan dengan pelanggan.Metode penelitian yang digunakan meliputi : objek penelititan, jenis dan sumber data, teknik pengumpulan data, dan metode pengembangan sistem.Laporan Tugas Akhir ini membahas tentang pengembangan sistem informasi penjualan yang meliputi : pendataan barang, pendataan pelanggan, proses transaksi panjualan serta pembuatan laporan, baik laporan pelanggan, laporan persediaan barang maupun laporan penjualan perperiode dan pernomor transaksi.Setelah rancangan selesai dibuat, maka dapat diketahui bahwa pada tahapan analisis sistem dengan menggunakan alat bantu alir dokumen (flow of document) dan rancangan desain sistem dengan menggunakan alat bantu diagram konteks (context diagram), diagram arus data (data flow diagram), diagram hubungan entitas (entity relationship diagram), teknik normalisasi (normalized), relasi antar tabel (table relationship), dan desain input output diharapakan penyimpanan data menjadi terpusat dan keberadaan data akan selalu terkontol dengan baik dari segi ketelitian dan validasi data dapat dipertanggungjawabkan sehingga informasi yang dihasilkan akan lebih baik.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708586bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df0092da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "laporan\n",
      "data\n",
      "penjualan\n",
      "sistem\n",
      "dan\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bdc5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c9379d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Berdasarkan Hasil Analisis yang telah dilakukan bahwa informasi yang disajikan selama ini dengan system yang manual akan sangat sulit untuk memberikan pelayanan yang optimal, karena masih terdapat beberapa kelemahan diantaranya masalah efisiensi, keakuratan data dan kecepatan pada waktu pencarian data. Hal ini akan sangat berpengaruh terhadap kesinambungan dan kelancaran jalannya pembayaran Rekening air bersih di PDAM Tirta Raharja Cabang I Cimahi.Untuk mengatasi hal tersebut perlu dibuat suatu rancangan system informasi dengan menggunakan alat-alat dan teknik-teknik perancangan system tersruktur dan tidak lupa untuk memperhatikan kebutuhan pemakaian system.Perancanga yang dilakukan meliputi pemilihan perangkat lunak dan perangkat keras. System yang diusulkan tidak mengubah tugas-tugas yang telah ada selama ini, nmaun hanya mengubah bagaimana cara tersebut diproses dengan mengotomatisasi pekerjaan manual yang telah ada. Hasil akhir penelitian ini adalah merupakan aplikasi yang diharapkan dapat membantu PDAM Tirta Raharja Cabang I Cimahi dengan pengembangan system informasi pembayaran Rekening Air Bersih.   \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0740af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0aed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "yang\n",
      "system\n",
      "dengan\n",
      "telah\n",
      "informasi\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "693b26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66509a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dewasa ini, teknologi berkembang dengan pesat dalam kehidupan sehari-hari. Dimana dengan adanya perkembangan teknologi tersebut memberikan keuntungan dan kerugian masing-masing. Salah satu keuntungannya adalah teknologi bisa mengontrol peralatan elektronik yang dapat dilakukan dengan menerapkan Internet of Things (IoT) pada lampu rumah. IoT Smart Home adalah kombinasi antara teknologi dan layanan yang digunakan untuk rumah dan memiliki fungsi khusus. Manfaat Smart Home dirancang untuk meningkatkan efisiensi, kemudahan dan keselamatan bagi pengguna. Sistem rumah pintar IoT terdiri dari peralatan monitoring, peralatan kendali, dan akses otomatis beberapa peralatan yang dapat diakses oleh komputer. Rumah pintar IoT biasanya terdiri dari peralatan kontrol, pemantauan dan otomatisasi berbagai perangkat atau peralatan rumah tangga yang dapat diakses melalui komputer. Penelitian ini menerapkan konsep Service Oriented Architecture (SOA) pada smart home dengan mengintegrasikan Rasberry Pi dengan pemograman phyton, web server (database) dengan pemograman PHP dan Android dengan pemograman java dan dalam kasus ini yang dikontrol adalah lampu rumah. Adapun hasil dari penelitian ini berupa prototype rumah, web service dan aplikasi Android SOA SMARTHOME dengan fitur 5 tombol untuk mengontrol/monitor lampu dan sistem ini saling terintegrasi dengan konsep Service Oriented Architecture yang dilengkapi dengan token sebagai keamanan sistem.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e9c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b97cab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "dan\n",
      "dengan\n",
      "yang\n",
      "rumah\n",
      "peralatan\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04b6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be233866",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Pada era perkembangan teknologi saat ini, sebuah informasi sangat dibutuhkan dalam kehidupan manusia dan sudah banyak digunakan dalam berbagai bidang salah satunya dalam bidang pendidikan di suatu perguruan tinggi. Studi kasus pada Program Studi Teknik Informatika Universitas Hasanuddin menujukkan bahwa telah menerapkan beberapa sistem informasi dalam membantu proses pekerjaan di kampus, namun belum secara keseluruhan. Masih terdapat beberapa pekerjaan yang belum menerapkan sistem informasi di dalamnya, salah satunya yaitu belum tersedia sistem informasi yang dapat membantu dalam proses pengumpulan proposal tugas akhir mahasiswa yang dimana masih menggunakan media email. Untuk itu pada penelitian ini dilakukan pengembangan perangkat lunak berupa website Sistem Penilaian Kelayakan Proposal Tugas Akhir Mahasiswa dengan Sistem Deteksi Plagiasi. Dengan adanya website ini, diharapkan dapat membantu institusi dalam mengelola proposal tugas akhir mahasiswa, dapat memudahkan admin dalam pembagian tugas bagi dosen untuk me-review proposal tugas akhir dan adapun pada website yang akan dibuat nantinya dilengkapi dengan fitur tambahan yaitu pengecekan similarity guna untuk mengetahui tingkat kemiripan pada proposal mahasiswa.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98f23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for sentence in sentences for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    # Remove stopwords and perform stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_word_scores(words, window_size=2):\n",
    "    word_freq = FreqDist(words)\n",
    "    word_scores = {}\n",
    "\n",
    "    for word in set(words):\n",
    "        word_scores[word] = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_scores[word] += word_freq[words[j]]\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "def textrank_keywords(text, top_n=5):\n",
    "    words = preprocess_text(text)\n",
    "    word_scores = calculate_word_scores(words)\n",
    "\n",
    "    # Sort the words based on their scores in descending order\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N keywords\n",
    "    top_keywords = [word for word, score in sorted_words[:top_n]]\n",
    "    return top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c5b6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords:\n",
      "akhir\n",
      "tuga\n",
      "propos\n",
      "dalam\n",
      "mahasiswa\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the top keywords (e.g., N=5)\n",
    "top_keywords = textrank_keywords(text, top_n=5)\n",
    "\n",
    "# Print the top keywords\n",
    "print(\"Top Keywords:\")\n",
    "for keyword in top_keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c967b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
